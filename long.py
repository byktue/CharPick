import os
import re
import json
import requests
import chardet  # ç”¨äºè‡ªåŠ¨æ£€æµ‹ç¼–ç 
import langextract as lx
from langextract.providers.ollama import OllamaLanguageModel

# --- 1. é…ç½®æœ¬åœ°æ¨¡å‹ä¸åœ°å€ ---
OLLAMA_URL = "http://localhost:11434"
MODEL_ID = "gemma2:9b"

local_model = OllamaLanguageModel(
    model_id=MODEL_ID,
    model_url=OLLAMA_URL
)

# --- 2. è‡ªåŠ¨ç¼–ç æ£€æµ‹ä¸åˆ‡åˆ†é€»è¾‘ ---
def advanced_split_novel(file_path):
    """
    1. è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç¼–ç  (GBK/UTF-8/Big5 ç­‰)
    2. é’ˆå¯¹ã€Šç¥æ¸¸ã€‹ç­‰ç½‘æ–‡ä¼˜åŒ–ï¼šè·³è¿‡éæ­£æ–‡å†…å®¹
    3. æ”¯æŒå¤šç§ç« èŠ‚åˆ‡åˆ†æ¨¡å¼
    """
    # ç¬¬ä¸€æ­¥ï¼šæ£€æµ‹ç¼–ç 
    with open(file_path, 'rb') as f:
        raw_data = f.read(1024 * 1024)  # è¯»å–å‰ 1MB å­—èŠ‚è¿›è¡Œæ£€æµ‹
        result = chardet.detect(raw_data)
        encoding = result['encoding']
        confidence = result['confidence']
        print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹ç¼–ç : {encoding} (ç½®ä¿¡åº¦: {confidence:.2f})")

    # ç¬¬äºŒæ­¥ï¼šæŒ‰æ£€æµ‹åˆ°çš„ç¼–ç è¯»å–å…¨æ–‡
    # å¦‚æœæ£€æµ‹å¤±è´¥ï¼Œé»˜è®¤å›é€€åˆ° gb18030 (ä¸­æ–‡ç½‘æ–‡å®¹é”™ç‡æœ€é«˜)
    if not encoding or confidence < 0.7:
        encoding = 'gb18030'

    with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
        content = f.read()

    # è‡ªåŠ¨å®šä½æ­£æ–‡èµ·ç‚¹ï¼ˆä» 001å› æˆ– ç¬¬xç«  å¼€å§‹ï¼‰
    start_match = re.search(r'(\d{3}å›|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+[ç« èŠ‚å›])', content)
    if start_match:
        content = content[start_match.start():]
    
    # åŒ¹é…è§„åˆ™
    pattern = r'(\d{3}å›|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+[ç« èŠ‚å›].*)'
    parts = re.split(pattern, content)
    
    chapter_list = []
    for i in range(1, len(parts), 2):
        title = parts[i].strip()
        body = parts[i+1].strip() if i+1 < len(parts) else ""
        if len(body) > 50:
            chapter_list.append({"title": title, "content": body})
    
    return chapter_list

# --- 3. å‘é‡åŒ–å‡½æ•° (Embedding) ---
def get_ollama_embedding(text):
    try:
        response = requests.post(
            f"{OLLAMA_URL}/api/embeddings",
            json={"model": MODEL_ID, "prompt": text[:2000]}
        )
        return response.json().get("embedding", [])
    except Exception as e:
        print(f"âš ï¸ å‘é‡åŒ–å¤±è´¥: {e}")
        return []

# --- 4. æå–é€»è¾‘å®šä¹‰ ---
EXTRACT_PROMPT = """
åˆ†æä»¥ä¸‹å°è¯´ç« èŠ‚ï¼Œè¯·æå–ï¼š
1. å…³é”®æƒ…èŠ‚ (Plot): ç”¨ä¸‰å¥è¯æ€»ç»“æœ¬ç« æ ¸å¿ƒå†²çªä¸è¿›å±•ã€‚
2. è§’è‰²åŠ¨æ€ (Characters): æœ¬ç« æ ¸å¿ƒè§’è‰²çš„çŠ¶æ€å˜åŒ–ã€‚
"""

EXAMPLES = [
    lx.data.ExampleData(
        text="001å›ï¼šçŸ³é‡ä»å°èƒ½çœ‹è§åˆ«äººçœ‹ä¸è§çš„ä¸œè¥¿ï¼Œä»–åœ¨æ‘å£é‡åˆ°äº†ç–¯ç–¯ç™«ç™«çš„é£å›å­ã€‚",
        extractions=[
            lx.data.Extraction(
                extraction_class="chapter_info",
                extraction_text="001å›å†…å®¹",
                attributes={
                    "plot_summary": "ä¸»è§’çŸ³é‡å±•ç¤ºé€šçµå¤©èµ‹ï¼Œå¹¶åœ¨æ‘å£ä¸å…³é”®äººç‰©é£å›å­åˆæ¬¡ç›¸é‡ã€‚",
                    "characters_found": [
                        {"name": "çŸ³é‡", "status": "å±•ç¤ºå¤©èµ‹", "secret": "æ‹¥æœ‰å¤©ç”Ÿé€šçµèƒ½åŠ›"},
                        {"name": "é£å›å­", "status": "ç¥ç§˜å‡ºåœº", "secret": "èº«ä»½ä¸æ˜çš„å¼•å¯¼è€…"}
                    ]
                }
            )
        ]
    )
]

# --- 5. æ‰¹é‡æµæ°´çº¿ ---
def run_charpick_production_pipeline(novel_path):
    print(f"ğŸ“– æ­£åœ¨å°è¯•è¯»å–å°è¯´: {novel_path}")
    try:
        chapters = advanced_split_novel(novel_path)
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return

    print(f"âœ‚ï¸ è§£æå®Œæˆï¼Œæœ‰æ•ˆç« èŠ‚å…± {len(chapters)} ç« ã€‚å¼€å§‹å¤„ç†...")

    results_file = "charpick_vector_database.jsonl"
    
    with open(results_file, "a", encoding="utf-8") as f_out:
        for idx, chapter in enumerate(chapters):
            chunk_text = f"{chapter['title']}\n{chapter['content'][:3000]}"
            print(f"ğŸš€ å¤„ç†ä¸­ [{idx+1}/{len(chapters)}]: {chapter['title']}")
            
            try:
                # A: ç»“æ„åŒ–æå–
                result = lx.extract(
                    text_or_documents=chunk_text,
                    prompt_description=EXTRACT_PROMPT,
                    examples=EXAMPLES,
                    model=local_model
                )
                
                extraction_data = result.extractions[0].attributes if result.extractions else {}
                plot_text = extraction_data.get("plot_summary", "")

                # B: å‘é‡åŒ–
                vector = []
                if plot_text:
                    vector = get_ollama_embedding(plot_text)

                # C: æ•´åˆä¿å­˜
                final_record = {
                    "id": idx,
                    "title": chapter['title'],
                    "metadata": extraction_data,
                    "vector": vector,
                    "raw_text_preview": chapter['content'][:200]
                }
                f_out.write(json.dumps(final_record, ensure_ascii=False) + "\n")
                
            except Exception as e:
                print(f"âŒ ç¬¬ {idx+1} ç« å¤„ç†å¤±è´¥: {e}")

    print(f"âœ¨ å…¨é‡å¤„ç†å®Œæˆï¼ç»“æœå·²å­˜å…¥: {results_file}")

if __name__ == "__main__":
    MY_NOVEL = os.path.join("data", "ç¥æ¸¸.txt")
    if os.path.exists(MY_NOVEL):
        run_charpick_production_pipeline(MY_NOVEL)
    else:
        print(f"âŒ æœªæ‰¾åˆ°æ–‡ä»¶: {MY_NOVEL}")